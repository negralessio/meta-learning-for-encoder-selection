{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import src.load_datasets as ld\n",
    "import src.pairwise_utils as pu\n",
    "from src.utils import load_config\n",
    "from src.feature_engineering import feature_selection, normalize_train_data, normalize_test_data\n",
    "from src.data_cleaning import drop_pearson_correlated_features\n",
    "from src.meta_information import add_dataset_meta_information\n",
    "from src.encoding import ohe_encode_train_data, ohe_encode_test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:11.372996200Z",
     "start_time": "2023-07-10T15:03:11.293933200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:11.423446Z",
     "start_time": "2023-07-10T15:03:11.327876Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from '../../data/raw/dataset_rank_train.csv' ...\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../../data/raw/\"\n",
    "\n",
    "FACTORS = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "NEW_INDEX = \"encoder\"\n",
    "cfg = load_config(\"../../configs/config.yaml\")\n",
    "\n",
    "df_train = ld.load_dataset(DATA_DIR + \"dataset_rank_train.csv\")\n",
    "#df_test = ld.load_dataset(DATA_DIR + \"dataset_rank_test.csv\")  # as usual, replace it with your own validation set\n",
    "\n",
    "X_train = df_train[FACTORS + [\"encoder\"]].groupby(FACTORS).agg(lambda x: np.nan).reset_index()[FACTORS]\n",
    "#X_test = df_test[FACTORS + [\"encoder\"]].groupby(FACTORS).agg(lambda x: np.nan).reset_index()[FACTORS]\n",
    "\n",
    "# join to ensure X_train and y_train's indices are ordered the same\n",
    "y_train = pd.merge(X_train,\n",
    "                   pu.get_pairwise_target(df_train, features=FACTORS, target=\"rank\", column_to_compare=\"encoder\"),\n",
    "                   on=FACTORS, how=\"left\").drop(FACTORS, axis=1).fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.573572Z",
     "start_time": "2023-07-10T15:03:11.352777200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "   dataset model tuning scoring\n0        3   DTC   full     ACC\n1        3   DTC   full     AUC\n2        3   DTC   full      F1\n3        3   DTC  model     AUC\n4        3   DTC  model      F1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>model</th>\n      <th>tuning</th>\n      <th>scoring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>DTC</td>\n      <td>full</td>\n      <td>ACC</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>DTC</td>\n      <td>full</td>\n      <td>AUC</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>DTC</td>\n      <td>full</td>\n      <td>F1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>DTC</td>\n      <td>model</td>\n      <td>AUC</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>DTC</td>\n      <td>model</td>\n      <td>F1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.589320800Z",
     "start_time": "2023-07-10T15:03:22.573572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train  (1161, 4)\n",
      "Shape of y_train  (1161, 992)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train \", X_train.shape)\n",
    "print(\"Shape of y_train \", y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.628840500Z",
     "start_time": "2023-07-10T15:03:22.589320800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline\n",
    "Run model with baseline data (no preprocessing)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# Preprocess data - baseline (one-hot encoding)\n",
    "scaler = OneHotEncoder()\n",
    "X_train_baseline = scaler.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.683954100Z",
     "start_time": "2023-07-10T15:03:22.605075600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Run model\n",
    "models = {\"\"}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.762318800Z",
     "start_time": "2023-07-10T15:03:22.628840500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model with Preprocessed Data\n",
    "Run model with preprocess data (full preprocessed pipeline)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "X_train_preprocessed = X_train.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.809123600Z",
     "start_time": "2023-07-10T15:03:22.636964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def preprocess_data_pairwise(X_train, y_train, X_test, cfg, verbosity):\n",
    "    # General encodings: One Hot Encode (OHE) subset of features\n",
    "    X_train, ohe = ohe_encode_train_data(X_train=X_train,\n",
    "                                         cols_to_encode=cfg[\"feature_engineering\"][\"features_to_ohe\"],\n",
    "                                         verbosity=verbosity)\n",
    "    if X_test is not None:\n",
    "        X_test = ohe_encode_test_data(X_test=X_test, cols_to_encode=cfg[\"feature_engineering\"][\"features_to_ohe\"],\n",
    "                                      ohe=ohe, verbosity=verbosity)\n",
    "\n",
    "    # Add dataset_agg (= csv-file containing meta information about the datasets)\n",
    "    # The file can be created with the notebook from week 09\n",
    "    print(\"Add dataset meta information...\")\n",
    "    X_train = add_dataset_meta_information(df=X_train,\n",
    "                                           path_to_meta_df=\"../../data/preprocessed/dataset_agg.csv\",\n",
    "                                           nan_threshold=cfg[\"feature_engineering\"][\"dataset_meta_information\"][\n",
    "                                               \"nan_threshold\"],\n",
    "                                           replacing_strategy=cfg[\"feature_engineering\"][\"dataset_meta_information\"][\n",
    "                                               \"replacing_strategy\"])\n",
    "    if X_test is not None:\n",
    "        X_test = add_dataset_meta_information(df=X_test,\n",
    "                                              path_to_meta_df=\"../../data/preprocessed/dataset_agg.csv\",\n",
    "                                              nan_threshold=cfg[\"feature_engineering\"][\"dataset_meta_information\"][\n",
    "                                                  \"nan_threshold\"],\n",
    "                                              replacing_strategy=cfg[\"feature_engineering\"][\"dataset_meta_information\"][\n",
    "                                                  \"replacing_strategy\"])\n",
    "\n",
    "    # Drop correlated features\n",
    "    X_train, X_test = drop_pearson_correlated_features(train_data=X_train,\n",
    "                                                       test_data=X_test,\n",
    "                                                       threshold=\n",
    "                                                       cfg[\"data_cleaning\"][\"pearson_correlation\"][\n",
    "                                                           \"threshold\"],\n",
    "                                                       verbosity=verbosity)\n",
    "\n",
    "    # Select features\n",
    "    X_train, X_test = feature_selection(X_train=X_train, X_test=X_test, y_train=y_train, quantile=0.4, verbosity=2)\n",
    "\n",
    "    # Normalize data\n",
    "    X_train, scaler = normalize_train_data(X_train=X_train, method=cfg[\"feature_engineering\"][\"normalize\"][\"method\"],\n",
    "                                           verbosity=verbosity)\n",
    "    if X_test is not None:\n",
    "        X_test = normalize_test_data(X_test=X_test, scaler=scaler, verbosity=verbosity)\n",
    "\n",
    "    return X_train, y_train, X_test, scaler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:22.887879200Z",
     "start_time": "2023-07-10T15:03:22.652604100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding the features ['model', 'tuning', 'scoring'] of the train data ...\n",
      "Add dataset meta information...\n",
      "Drop pearson correlated features with threshold 0.7...\n",
      "Filter correlated features\n",
      "Feature selection...\n",
      "Normalizing train data using method 'minmax' ...\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 655 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_preprocessed, y_train_preprocessed, X_test_preprocessed, scaler = preprocess_data_pairwise(X_train=X_train_preprocessed, y_train=y_train, X_test=None, cfg=cfg, verbosity=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:23.365379Z",
     "start_time": "2023-07-10T15:03:22.699584200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T15:03:23.404079200Z",
     "start_time": "2023-07-10T15:03:23.365379Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
