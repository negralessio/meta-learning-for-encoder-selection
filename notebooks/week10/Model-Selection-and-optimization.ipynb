{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8975adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eacf5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeRegressor\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a36aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prequirements:\n",
    "#   - Preprocessed data is loaded\n",
    "#   - Check the dataset for \n",
    "#     - Data types (only numerical)\n",
    "#     - Null values\n",
    "#     - Normalization\n",
    "\n",
    "# Define: \n",
    "#   - Model\n",
    "#   - Feature selection\n",
    "#   - Hyperparameter grid\n",
    "\n",
    "# Perform: \n",
    "#   - Correlation analysis and dropping of highly correlated features\n",
    "#   - Feature selection\n",
    "#   - Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4204db",
   "metadata": {},
   "source": [
    "# Load preprocessed data and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21604b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Literal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from gensim.models.poincare import PoincareModel\n",
    "\n",
    "\n",
    "def load_graph(path: Union[Path, str]) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Load a graph from a file. The file must be in the format of an adjacency list.\n",
    "    \n",
    "    :param path: Path to the file containing the graph.\n",
    "    :type path: Union[Path, str]\n",
    "\n",
    "    :return: The graph.\n",
    "    \"\"\"\n",
    "    G = nx.read_adjlist(path)\n",
    "\n",
    "    # add node names as labels to the graph\n",
    "    node_names = {node: node for node in G.nodes()}\n",
    "    nx.set_node_attributes(G, node_names, \"label\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def poincare_encoding(path_to_embeddings: str, path_to_graph: str = None, data=None, column_to_encode=None,\n",
    "                      dim_reduction: Optional[Literal['pca', 'tsne']] = None, n_components=2, encode_dim=50, epochs=500, seed=7,\n",
    "                      explode_dim=True, verbosity=1) -> Union[pd.DataFrame, tuple[pd.DataFrame, PoincareModel]]:\n",
    "    \"\"\"\n",
    "    Generates the PoincarÃ¨ embedding for the given graph and encodes the given column of the given data with it. The\n",
    "    encoding can be done in different formats. The function can also be used to just generate the embedding for the\n",
    "    given graph. The graph has to be given as an edge list.\n",
    "\n",
    "    :param dim_reduction: Dimensionality reduction method to use. Either 'pca' or 'tsne'. If None, no dimensionality\n",
    "        reduction is applied.\n",
    "    :type dim_reduction: Optional[Literal['pca', 'tsne']]\n",
    "    :param n_components: Number of components to reduce the dimensionality to.\n",
    "    :type n_components: int\n",
    "    :param path_to_graph: Path to the graph.\n",
    "    :type path_to_graph: str\n",
    "    :param path_to_embeddings: Path to the embeddings.\n",
    "    :type path_to_embeddings: str\n",
    "    :param data: Data to encode.\n",
    "    :type data: pandas.DataFrame\n",
    "    :param column_to_encode: Column to encode.\n",
    "    :type column_to_encode: str\n",
    "    :param encode_dim: Dimension of the embedding.\n",
    "    :type encode_dim: int\n",
    "    :param epochs: Number of epochs to train the model.\n",
    "    :type epochs: int\n",
    "    :param seed: Seed for the random number generator.\n",
    "    :type seed: int\n",
    "    :param explode_dim: If True, the embedding is exploded into multiple columns.\n",
    "    :type explode_dim: bool\n",
    "    :param verbosity: Verbosity level.\n",
    "    :type verbosity: int\n",
    "\n",
    "    :return: The encoded data.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    if path_to_graph is None:\n",
    "        # Load the embeddings\n",
    "        if verbosity > 0:\n",
    "            print(f\"Loading the embeddings from '{path_to_embeddings}'...\")\n",
    "        emb_df = pd.read_csv(path_to_embeddings, index_col=0)\n",
    "        model = None\n",
    "    else:\n",
    "        # Load Graph\n",
    "        G = load_graph(path_to_graph)\n",
    "        # Embed the graph\n",
    "        if verbosity > 0:\n",
    "            print(\"(Poincare) Embedding the graph ...\")\n",
    "        model = PoincareModel(list(G.edges()), seed=seed, size=encode_dim)\n",
    "        model.train(epochs=epochs, print_every=500)\n",
    "        # Get the embeddings and map them to the node names\n",
    "        embeddings_dict = {node: model.kv[node] for node in G.nodes}\n",
    "        emb_df = pd.DataFrame.from_dict(embeddings_dict, orient='index')\n",
    "        if path_to_embeddings is not None:\n",
    "            # Save the embeddings\n",
    "            if verbosity > 1:\n",
    "                print(f\"Saving the embeddings to '{path_to_embeddings}'...\")\n",
    "            emb_df.to_csv(path_to_embeddings)\n",
    "\n",
    "    if dim_reduction == 'pca':\n",
    "        # Reduce the dimensionality of the embeddings\n",
    "        if verbosity > 1:\n",
    "            print(\"Reducing the dimensionality of the embeddings by applying PCA...\")\n",
    "        pca = PCA(n_components=n_components, random_state=seed)\n",
    "        emb_df = pd.DataFrame(pca.fit_transform(emb_df), index=emb_df.index)\n",
    "    elif dim_reduction == 'tsne':\n",
    "        # Reduce the dimensionality of the embeddings\n",
    "        if verbosity > 1:\n",
    "            print(\"Reducing the dimensionality of the embeddings by applying t-SNE...\")\n",
    "        tsne = TSNE(n_components=n_components, random_state=seed)\n",
    "        emb_df = pd.DataFrame(tsne.fit_transform(emb_df), index=emb_df.index)\n",
    "\n",
    "    if data is None or column_to_encode is None:\n",
    "        return emb_df, model\n",
    "    else:\n",
    "        if verbosity > 0:\n",
    "            print(f\"Encoding the data feature '{column_to_encode}'...\")\n",
    "        if explode_dim:\n",
    "            # Rename the columns to enc_dim_0, enc_dim_1, ...\n",
    "            emb_df.columns = [f'enc_dim_{col}' for col in emb_df.columns]\n",
    "            # Merge the embeddings with the data\n",
    "            encoded_data_df = data.merge(emb_df, left_on=column_to_encode, right_index=True, how='left')\n",
    "        else:\n",
    "            # Combine the embeddings into one column\n",
    "            emb_df['combined_enc_emb'] = emb_df.values.tolist()\n",
    "            encoded_data_df = data.merge(emb_df['combined_emb'], left_on=column_to_encode, right_index=True, how='left')\n",
    "        # Drop the column to encode\n",
    "        encoded_data_df.drop(column_to_encode, axis=1, inplace=True)\n",
    "        return encoded_data_df, model\n",
    "\n",
    "\n",
    "def ohe_encode_train_data(X_train: pd.DataFrame, cols_to_encode: list, verbosity=1) -> (pd.DataFrame, OneHotEncoder):\n",
    "    \"\"\"\n",
    "    Function to One Hot Encode the train data: Fits and transforms the OHE Object on the train data;\n",
    "    more specifically: The provided cols_to_encode (list of features). Function also makes sure that a\n",
    "    pd.DataFrame is returned by dropping the old features and concatenating the encoded ones.\n",
    "\n",
    "    :param X_train: pd.DataFrame -- Provided Train Dataset\n",
    "    :param cols_to_encode: list -- Provided list of features to apply OHE on\n",
    "    :param verbosity: int -- Level of verbosity\n",
    "\n",
    "    :return: Tuple with pd.DataFrame with encoded features and fitted OHE object\n",
    "    \"\"\"\n",
    "    if verbosity > 0:\n",
    "        print(f\"One Hot Encoding the features {cols_to_encode} of the train data ...\")\n",
    "\n",
    "    # Get DataFrame with only relevant features, i.e. cols_to_encode\n",
    "    X_train_cats = X_train[cols_to_encode]\n",
    "\n",
    "    # Fit OneHotEncoding object\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", dtype=np.float32)\n",
    "    X_train_cats_encoded = ohe.fit_transform(X_train_cats).toarray()\n",
    "\n",
    "    # Transform encoded data to pandas dataframe\n",
    "    X_train_cats_encoded = pd.DataFrame(X_train_cats_encoded, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "    # Drop old features\n",
    "    feats_to_drop = list(ohe.feature_names_in_)\n",
    "    X_train = X_train.drop(columns=feats_to_drop, axis=1)\n",
    "\n",
    "    # Concat old dataframe with new encoded features\n",
    "    X_train_encoded = pd.concat([X_train, X_train_cats_encoded], axis=1)\n",
    "\n",
    "    return X_train_encoded, ohe\n",
    "\n",
    "\n",
    "def ohe_encode_test_data(X_test: pd.DataFrame, cols_to_encode: list, ohe: OneHotEncoder, verbosity=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to apply the fitted OHE object on the test set features provided in param cols_to_encode.\n",
    "    Also makes sure that pd.DataFrame is returned by dropping the old features and concatenating the encoded ones.\n",
    "\n",
    "    :param X_test: pd.DataFrame -- Provided Test Dataset\n",
    "    :param cols_to_encode: list -- Provided list of features to apply OHE on\n",
    "    :param ohe: OneHotEncoder -- Fitted OHE object\n",
    "    :param verbosity: int -- Level of verbosity\n",
    "\n",
    "    :return: pd.DataFrame -- Encoded Test Dataset\n",
    "    \"\"\"\n",
    "    if verbosity > 0:\n",
    "        print(f\"One Hot Encoding the features {cols_to_encode} of the test data ...\")\n",
    "\n",
    "    # Get DataFrame with only relevant features, i.e. cols_to_encode and transform them\n",
    "    X_test_cats = X_test[cols_to_encode]\n",
    "    X_test_cats_encoded = ohe.transform(X_test_cats).toarray()\n",
    "\n",
    "    # Transform to pandas DataFrame\n",
    "    X_test_cats_encoded = pd.DataFrame(X_test_cats_encoded, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "    # Drop old features\n",
    "    feats_to_drop = list(ohe.feature_names_in_)\n",
    "    X_test = X_test.drop(columns=feats_to_drop, axis=1)\n",
    "\n",
    "    # Concat old dataframe with new encoded features\n",
    "    X_test_encoded = pd.concat([X_test, X_test_cats_encoded], axis=1)\n",
    "\n",
    "    return X_test_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8bfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_validated_indices(df: pd.DataFrame, factors: Iterable[str], target: str,\n",
    "                                   **kfoldargs) -> List[List[Iterable[int]]]:\n",
    "    df_factors = df.groupby(factors)[target].mean().reset_index()\n",
    "    X_factors, y_factors = df_factors.drop(target, axis=1), df_factors[target]\n",
    "\n",
    "    indices = []\n",
    "    for itr, ite in KFold(**kfoldargs).split(X_factors, y_factors):\n",
    "        tr = pd.merge(X_factors.iloc[itr], df.reset_index(), on=factors)[\"index\"]  # \"index\" is the index of df\n",
    "        te = pd.merge(X_factors.iloc[ite], df.reset_index(), on=factors)[\"index\"]  # \"index\" is the index of df\n",
    "        indices.append([tr, te])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d975455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data ...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.load_datasets import load_dataset, load_rankings, load_train_data\n",
    "\n",
    "FACTORS = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "NEW_INDEX = \"encoder\"\n",
    "TARGET = \"cv_score\"\n",
    "\n",
    "X_train, y_train = load_train_data('../../data/raw/dataset_train.csv')\n",
    "indices = custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1).copy(), FACTORS, TARGET, n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d36a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Poincare) Embedding the graph ...\n",
      "Saving the embeddings to '../../data/preprocessed/embeddings.csv'...\n",
      "Encoding the data feature 'encoder'...\n"
     ]
    }
   ],
   "source": [
    "import src.encoding\n",
    "from src.meta_information import add_dataset_meta_information\n",
    "\n",
    "X_train, _ = poincare_encoding(path_to_graph=\"../../data/raw/graph.adjlist\",\n",
    "                                            path_to_embeddings=\"../../data/preprocessed/embeddings.csv\",\n",
    "                                            data=X_train,\n",
    "                                            column_to_encode=\"encoder\",\n",
    "                                            encode_dim=50,\n",
    "                                            explode_dim=True,\n",
    "                                            epochs=5000,\n",
    "                                            dim_reduction=False,\n",
    "                                            verbosity=2)\n",
    "\n",
    "X_train = add_dataset_meta_information(df=X_train,\n",
    "                                       path_to_meta_df=\"../../data/preprocessed/dataset_agg.csv\",\n",
    "                                       nan_threshold=0.4,\n",
    "                                       replacing_strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ca2ef",
   "metadata": {},
   "source": [
    "# Check dataset\n",
    "\n",
    "- Data types (only numerical)\n",
    "- Null values\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226ae134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are columns which are not encoded: {'scoring', 'tuning', 'model'}\n"
     ]
    }
   ],
   "source": [
    "all_columns = X_train.columns\n",
    "numeric_columns = X_train.select_dtypes(include=np.number).columns\n",
    "\n",
    "encode = False\n",
    "scale = False\n",
    "\n",
    "# Check if there are only numeric columns\n",
    "if len(all_columns) > len(numeric_columns):\n",
    "    encode_cols = set(all_columns) - set(numeric_columns)\n",
    "    encode = True\n",
    "    print(f\"There are columns which are not encoded: {encode_cols}\")\n",
    "    \n",
    "\n",
    "# Check if there are null values, if yes, fill them with the mean\n",
    "if X_train.isna().any().sum() > 0:\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "#if X_val.isna().any().sum() > 0:\n",
    "#    X_val = X_val.fillna(X_val.mean())\n",
    "    \n",
    "    \n",
    "# Check if data is normalized to [0, 1]\n",
    "#if max(X_train.max()) > 1 or min(X_train.min()) < 0:\n",
    "#    scale = True\n",
    "#    print(\"Dataframe will be scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a8add",
   "metadata": {},
   "source": [
    "# Define \n",
    "\n",
    "- Model\n",
    "- Feature selection (https://machinelearningmastery.com/feature-selection-for-regression-data/)\n",
    "- Hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4742a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abefaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__max_depth\": [5, 10, 15, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3e57d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearson_correlated_features(data=None, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Calculates the pearson correlation of all features in the dataframe and returns a set of features with a\n",
    "    correlation greater than the threshold.\n",
    "\n",
    "    :param data: The input dataframe.\n",
    "    :type data: pd.DataFrame\n",
    "    :param threshold: The threshold for the correlation coefficient in the range of [0.0, 1.0].\n",
    "    :type threshold: float,optional(default=0.7)\n",
    "\n",
    "    :return: The set of features with a correlation greater than the threshold.\n",
    "    :rtype: set\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    # Get the set of correlated features\n",
    "    correlated_features = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "\n",
    "    return correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948de2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_features(X_train, y_train, quantile):\n",
    "    # configure to select a subset of features\n",
    "    fs = SelectKBest(score_func=f_regression, k='all')  # or mutual_info_regression\n",
    "    fs.fit(X_train, y_train)\n",
    "    \n",
    "    # Select columns based on mask\n",
    "    mask = [x >= np.quantile(fs.scores_, quantile) for x in fs.scores_]  # 0.4\n",
    "    X_train_fs = X_train.loc[:, mask]\n",
    "    #X_val_fs = X_val.loc[:, mask]\n",
    "    \n",
    "    #fs = SelectKBest(score_func=f_regression, k=k)\n",
    "    # learn relationship from training data\n",
    "    #fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    #X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    #X_test_fs = fs.transform(X_test)\n",
    "    \n",
    "    return list(X_train_fs.columns)\n",
    "    #return pd.DataFrame(X_train_fs), pd.DataFrame(X_test_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f89bf",
   "metadata": {},
   "source": [
    "# Perform\n",
    "\n",
    "- Correlation analysis and dropping of highly correlated features\n",
    "- Feature selection\n",
    "- Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b0a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop correlated features: \n",
      " {'MaxAttributeEntropy', 'Quartile2MutualInformation', 'MeanMutualInformation', 'StdvNominalAttDistinctValues', 'MinAttributeEntropy', 'enc_dim_15', 'PercentageOfMissingValues', 'enc_dim_22', 'non_categorical_features_count', 'enc_dim_40', 'MaxMutualInformation', 'total_feature_count', 'MinorityClassSize', 'enc_dim_28', 'enc_dim_36', 'MaxNominalAttDistinctValues', 'enc_dim_29', 'enc_dim_48', 'enc_dim_10', 'avg_number_of_categories_per_cat_feature', 'MajorityClassPercentage', 'row_count', 'null_value_count', 'ratio_of_null_values_to_all', 'MeanAttributeEntropy', 'enc_dim_7', 'enc_dim_2', 'Quartile2AttributeEntropy', 'enc_dim_41', 'enc_dim_43', 'enc_dim_47', 'enc_dim_30', 'EquivalentNumberOfAtts', 'enc_dim_13', 'NumberOfSymbolicFeatures', 'ratio_of_categorical_features_to_all', 'enc_dim_49', 'sum_of_all_categories', 'enc_dim_12', 'enc_dim_44', 'column_count', 'enc_dim_46', 'enc_dim_16', 'enc_dim_5', 'columns_with_null_values_count', 'enc_dim_35'}\n"
     ]
    }
   ],
   "source": [
    "# Drop correlated features\n",
    "column_list = set(X_train.columns) - set([\"model\", \"tuning\", \"scoring\"])\n",
    "\n",
    "correlated_features = get_pearson_correlated_features(data=X_train[list(column_list)], threshold=0.8)\n",
    "print(\"Drop correlated features: \\n\", correlated_features)\n",
    "X_train_clean = X_train.drop(correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1dd3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before selection: 58\n",
      "Number of features after selection : 33\n",
      "['enc_dim_11', 'PercentageOfSymbolicFeatures', 'enc_dim_20', 'MinMutualInformation', 'MeanNominalAttDistinctValues', 'categorical_features_count', 'enc_dim_8', 'AutoCorrelation', 'rows_with_null_values_count', 'PercentageOfInstancesWithMissingValues', 'enc_dim_45', 'PercentageOfNumericFeatures', 'MajorityClassSize', 'ClassEntropy', 'Quartile1MutualInformation', 'Dimensionality', 'max_number_of_categories_per_cat_feature', 'Quartile3AttributeEntropy', 'MinNominalAttDistinctValues', 'NumberOfBinaryFeatures', 'enc_dim_42', 'MinorityClassPercentage', 'Quartile1AttributeEntropy', 'min_number_of_categories_per_cat_feature', 'enc_dim_34', 'enc_dim_17', 'enc_dim_23', 'enc_dim_6', 'dataset', 'PercentageOfBinaryFeatures', 'NumberOfNumericFeatures', 'Quartile3MutualInformation', 'enc_dim_14']\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "print(f\"Number of features before selection: {X_train_clean.shape[1]}\")\n",
    "\n",
    "column_list = set(X_train_clean.columns) - set([\"model\", \"tuning\", \"scoring\"])\n",
    "selected_feature_set = select_features(X_train_clean[list(column_list)], y_train.values.ravel(), 0.4)\n",
    "\n",
    "print(f\"Number of features after selection : {len(selected_feature_set)}\")\n",
    "print(selected_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb604e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'model__max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# Perform GridSearchCV\n",
    "pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"model\", model)])\n",
    "#indices = custom_cross_validated_indices(X_train_clean, FACTORS, TARGET, n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "column_list = set(X_train_clean.columns) - set([\"model\", \"tuning\", \"scoring\"])\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid, cv=indices, scoring=\"r2\").fit(X_train_clean[list(column_list)], y_train)\n",
    "\n",
    "print(\"Best parameters: \")\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7303acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With score: 0.4479765366422243 +/- 0.047288720620854124\n"
     ]
    }
   ],
   "source": [
    "print(f\"With score: {gs.best_score_} +/- {gs.cv_results_['std_test_score'][gs.cv_results_['mean_test_score'].argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc79494",
   "metadata": {},
   "source": [
    "# Runs for different models and parameters\n",
    "\n",
    "- Define a functions with the above functionality\n",
    "- Parameters for\n",
    "  - Model\n",
    "  - Parameter grid\n",
    "  - Feature selection\n",
    "  - Correlation (yes / no / threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "960ad918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(X_train=None,\n",
    "                     y_train=None,\n",
    "                     indices=None,\n",
    "                     model=None, \n",
    "                     parameter_grid=None, \n",
    "                     feature_selection=False, \n",
    "                     qunatile=0.4,\n",
    "                     drop_correlated_features=True, \n",
    "                     corr_threshold=0.8):\n",
    "    # If you want to calculate spearman you need the inital features, so keep \n",
    "    # column_list = set(X_train_clean.columns) - set([\"model\", \"tuning\", \"scoring\"])\n",
    "    # out of the removals\n",
    "    column_list = list(set(X_train_clean.columns) - set([\"model\", \"tuning\", \"scoring\"]))\n",
    "    \n",
    "    if drop_correlated_features:\n",
    "        # Calculate correlation matrix\n",
    "        \n",
    "        corr_matrix = X_train[column_list].corr()\n",
    "\n",
    "        # Get the set of correlated features\n",
    "        correlated_features = set()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > corr_threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    correlated_features.add(colname)\n",
    "\n",
    "        X_train = X_train.drop(correlated_features, axis=1)\n",
    "    \n",
    "    if feature_selection: \n",
    "        fs = SelectKBest(score_func=f_regression, k='all')  # or mutual_info_regression\n",
    "        fs.fit(X_train, y_train)\n",
    "\n",
    "        # Select columns based on mask\n",
    "        mask = [x >= np.quantile(fs.scores_, quantile) for x in fs.scores_]  # 0.4\n",
    "        X_train_fs = X_train.loc[:, mask]\n",
    "        selected_features = list(X_train_fs.columns)\n",
    "        X_train = X_train[selected_features]\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"model\", model)])\n",
    "    #indices = custom_cross_validated_indices(X_train_clean, FACTORS, TARGET, n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    column_list = list(set(X_train.columns) - set([\"model\", \"tuning\", \"scoring\"]))\n",
    "    gs = GridSearchCV(pipeline, param_grid, cv=indices, scoring=\"r2\").fit(X_train[column_list], y_train)\n",
    "\n",
    "    print(\"Best parameters: \")\n",
    "    print(gs.best_params_)\n",
    "    print(f\"With score: {gs.best_score_} +/- {gs.cv_results_['std_test_score'][gs.cv_results_['mean_test_score'].argmax()]}\")\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a99a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point indices are given, so define models and parameter grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971a32a",
   "metadata": {},
   "source": [
    "### Which models do I want to test? \n",
    "\n",
    "- DecisionTree\n",
    "- RandomForest\n",
    "- XGBoost\n",
    "- LGBM\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e100218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_rf = {\n",
    "    \"model__n_estimators\": [25, 50, 100, 150, 200],\n",
    "    \"model__max_depth\"   : [10, 25, 50, 75, None], \n",
    "    \"model__min_samples_split\": [2, 5, 10, 15, 20, 25],\n",
    "    \"model__min_samples_leaf\" : [1, 2, 5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e07ece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'model__max_depth': 10}\n",
      "With score: 0.44813157695338574 +/- 0.04703969013048095\n"
     ]
    }
   ],
   "source": [
    "gs = parameter_search(X_train=X_train,\n",
    "                      y_train=y_train,\n",
    "                      indices=indices,\n",
    "                      model=model, \n",
    "                      parameter_grid=grid_rf, \n",
    "                      feature_selection=False, \n",
    "                      qunatile=0.4,\n",
    "                      drop_correlated_features=True, \n",
    "                      corr_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14e229c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(random_state=42)\n",
    "\n",
    "grid_xgb = {\n",
    "    \"model__n_estimators\"     : [25, 50, 100, 150, 200],\n",
    "    \"model__max_depth\"        : [10, 25, 50, 75, None], \n",
    "    \"model__max_leaves\"       : [1, 5, 10, 25, 50, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52b8e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: \n",
      "{'model__max_depth': 5}\n",
      "With score: 0.44848212768752055 +/- 0.04685683933957084\n"
     ]
    }
   ],
   "source": [
    "gs = parameter_search(X_train=X_train,\n",
    "                      y_train=y_train,\n",
    "                      indices=indices,\n",
    "                      model=model, \n",
    "                      parameter_grid=grid_xgb, \n",
    "                      feature_selection=False, \n",
    "                      qunatile=0.4,\n",
    "                      drop_correlated_features=True, \n",
    "                      corr_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c859ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 4, 6, 8],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 4, 6, 8],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False],\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
