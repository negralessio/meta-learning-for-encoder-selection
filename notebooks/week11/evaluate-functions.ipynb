{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6471e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93dbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.load_datasets import load_dataset, load_rankings, load_train_data\n",
    "import src.evaluate_regression\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c1af8",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282fe56",
   "metadata": {},
   "source": [
    "## Given functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cae8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_validated_indices(df: pd.DataFrame, factors: Iterable[str], target: str,\n",
    "                                   **kfoldargs) -> List[List[Iterable[int]]]:\n",
    "    df_factors = df.groupby(factors)[target].mean().reset_index()\n",
    "    X_factors, y_factors = df_factors.drop(target, axis=1), df_factors[target]\n",
    "\n",
    "    indices = []\n",
    "    for itr, ite in KFold(**kfoldargs).split(X_factors, y_factors):\n",
    "        tr = pd.merge(X_factors.iloc[itr], df.reset_index(), on=factors).index  # \"index\" is the index of df\n",
    "        te = pd.merge(X_factors.iloc[ite], df.reset_index(), on=factors).index  # \"index\" is the index of df\n",
    "        indices.append([tr, te])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a94939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rankings(df: pd.DataFrame, factors, new_index, target) -> pd.DataFrame:\n",
    "\n",
    "    assert set(factors).issubset(df.columns)\n",
    "\n",
    "    rankings = {}\n",
    "    for group, indices in df.groupby(factors).groups.items():\n",
    "        score = df.iloc[indices].set_index(new_index)[target]\n",
    "        rankings[group] = score2ranking(score, ascending=False)\n",
    "\n",
    "    return pd.DataFrame(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d421002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2ranking(score: pd.Series, ascending=True):\n",
    "    \"\"\"\n",
    "    Ascending =\n",
    "        True: lower score = better rank (for instance, if score is the result of a loss function or a ranking itself)\n",
    "        False: greater score = better rank (for instance, if score is the result of a score such as roc_auc_score)\n",
    "    \"\"\"\n",
    "    c = 1 if ascending else -1\n",
    "    order_map = {\n",
    "        s: sorted(score.unique(), key=lambda x: c * x).index(s) for s in score.unique()\n",
    "    }\n",
    "    return score.map(order_map)\n",
    "\n",
    "\n",
    "def spearman_rho(x: Iterable, y: Iterable, nan_policy=\"omit\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)\n",
    "        return spearmanr(x, y, nan_policy=nan_policy)[0]\n",
    "\n",
    "\n",
    "def list_spearman(rf1: pd.DataFrame, rf2: pd.DataFrame) -> np.array:\n",
    "    if not rf1.columns.equals(rf2.columns) or not rf1.index.equals(rf2.index):\n",
    "        raise ValueError(\"The two input dataframes should have the same index and columns.\")\n",
    "\n",
    "    return np.array([\n",
    "        spearman_rho(r1, r2, nan_policy=\"omit\") for (_, r1), (_, r2) in zip(rf1.items(), rf2.items())\n",
    "    ])\n",
    "\n",
    "\n",
    "def average_spearman(rf1: pd.DataFrame, rf2: pd.DataFrame) -> np.array:\n",
    "    #with warnings.catch_warnings():\n",
    "    #    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    return np.nanmean(list_spearman(rf1, rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaad93c",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f1201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_cross_validated_indices(df: pd.DataFrame, factors, target: str,\n",
    "                                   **kfoldargs):\n",
    "    df_factors = df.groupby(factors)[target].mean().reset_index()\n",
    "    X_factors, y_factors = df_factors.drop(target, axis=1), df_factors[target]\n",
    "\n",
    "    indices = []\n",
    "    for itr, ite in KFold(**kfoldargs).split(X_factors, y_factors):\n",
    "        tr = pd.merge(X_factors.iloc[itr], df.reset_index(), on=factors)['index']  # \"index\" is the index of df\n",
    "        te = pd.merge(X_factors.iloc[ite], df.reset_index(), on=factors)['index']  # \"index\" is the index of df\n",
    "        indices.append([tr, te])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84cb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_rankings(df: pd.DataFrame, factors, new_index, target) -> pd.DataFrame:\n",
    "\n",
    "    assert set(factors).issubset(df.columns)\n",
    "\n",
    "    rankings = {}\n",
    "    #print(df.shape)\n",
    "    for group, indices in df.groupby(factors).groups.items():\n",
    "        #print(indices)\n",
    "        #print(df.shape)\n",
    "        score = df.loc[indices].set_index(new_index)[target]\n",
    "        rankings[group] = src.evaluate_regression.score2ranking(score, ascending=False)\n",
    "\n",
    "    return pd.DataFrame(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5bcc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fold_indices(indices):\n",
    "    for i in range(len(indices)):\n",
    "        # Train set\n",
    "        print(f\"Fold: {i}\")\n",
    "        print(f\"Number of indices in train: {len(indices[i][0])}\")\n",
    "        print(f\"Number of indices in test : {len(indices[i][1])}\")\n",
    "\n",
    "\n",
    "        intersect = np.intersect1d(indices[i][0], indices[i][1])\n",
    "        print(f\"Number of equal indices: {len(intersect)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea94310",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30235d",
   "metadata": {},
   "source": [
    "## Given functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a0af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from '../../data/raw/dataset_rank_train.csv'...\n",
      "0.9891831469088159\n",
      "0.9879437888849684\n",
      "0.9877661360418818\n",
      "0.9890122937636548\n",
      "0.9891292937952844\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_train_data('../../data/raw/dataset_rank_train.csv')\n",
    "X_train = X_train.drop(\"rank\", axis=1)\n",
    "\n",
    "# Define variables for ranking\n",
    "factors = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "new_index = \"encoder\"\n",
    "target = \"cv_score\"\n",
    "\n",
    "# Get indices for CV\n",
    "indices = custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "#indices = my_custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "\n",
    "for fold in indices:\n",
    "    # Define data of current fold\n",
    "    X_train_fold = X_train.iloc[fold[0]]\n",
    "    X_test_fold = X_train.iloc[fold[1]]\n",
    "    y_train_fold = y_train.iloc[fold[0]]\n",
    "    y_test_fold = y_train.iloc[fold[1]]\n",
    "    \n",
    "    # Train model and predict\n",
    "    dummy_pipe = Pipeline([(\"encoder\", OneHotEncoder()), (\"model\", DecisionTreeRegressor())])\n",
    "    y_pred = pd.Series(dummy_pipe.fit(X_train_fold, y_train_fold).predict(X_test_fold), index=y_test_fold.index, name=\"cv_score_pred\")\n",
    "    df_pred = pd.concat([X_test_fold, y_test_fold, y_pred], axis=1)\n",
    "    \n",
    "    # Rankings and avg_spearman\n",
    "    rankings_test = get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score\")\n",
    "    rankings_pred = get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score_pred\")\n",
    "    print(src.evaluate_regression.average_spearman(rankings_test, rankings_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b0cdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RangeIndex(start=0, stop=28876, step=1),\n",
       " RangeIndex(start=0, stop=7178, step=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show indices\n",
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073049d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Number of indices in train: 28876\n",
      "Number of indices in test : 7178\n",
      "Number of equal indices: 7178\n",
      "Fold: 1\n",
      "Number of indices in train: 28807\n",
      "Number of indices in test : 7247\n",
      "Number of equal indices: 7247\n",
      "Fold: 2\n",
      "Number of indices in train: 28910\n",
      "Number of indices in test : 7144\n",
      "Number of equal indices: 7144\n",
      "Fold: 3\n",
      "Number of indices in train: 28805\n",
      "Number of indices in test : 7249\n",
      "Number of equal indices: 7249\n",
      "Fold: 4\n",
      "Number of indices in train: 28818\n",
      "Number of indices in test : 7236\n",
      "Number of equal indices: 7236\n"
     ]
    }
   ],
   "source": [
    "validate_fold_indices(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126119b",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d0d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from '../../data/raw/dataset_rank_train.csv'...\n",
      "0.4250098422065961\n",
      "0.5003065019900456\n",
      "0.4722530960922732\n",
      "0.48726084567346123\n",
      "0.5030920623852049\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_train_data('../../data/raw/dataset_rank_train.csv')\n",
    "X_train = X_train.drop(\"rank\", axis=1)\n",
    "\n",
    "# Define variables for ranking\n",
    "factors = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "new_index = \"encoder\"\n",
    "target = \"cv_score\"\n",
    "\n",
    "# Get indices for CV\n",
    "#indices = custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "indices = my_custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "\n",
    "for fold in indices:\n",
    "    # Define data of current fold\n",
    "    X_train_fold = X_train.iloc[fold[0]]\n",
    "    X_test_fold = X_train.iloc[fold[1]]\n",
    "    y_train_fold = y_train.iloc[fold[0]]\n",
    "    y_test_fold = y_train.iloc[fold[1]]\n",
    "    \n",
    "    # Train model and predict\n",
    "    dummy_pipe = Pipeline([(\"encoder\", OneHotEncoder()), (\"model\", DecisionTreeRegressor())])\n",
    "    y_pred = pd.Series(dummy_pipe.fit(X_train_fold, y_train_fold).predict(X_test_fold), index=y_test_fold.index, name=\"cv_score_pred\")\n",
    "    df_pred = pd.concat([X_test_fold, y_test_fold, y_pred], axis=1)\n",
    "    \n",
    "    # Rankings and avg_spearman\n",
    "    rankings_test = my_get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score\")\n",
    "    rankings_pred = my_get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score_pred\")\n",
    "    print(src.evaluate_regression.average_spearman(rankings_test, rankings_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f0b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     16858\n",
      "1     16859\n",
      "2     16860\n",
      "3     16861\n",
      "4     16862\n",
      "5     16863\n",
      "6     16864\n",
      "7     16865\n",
      "8     16866\n",
      "9     16867\n",
      "10    16868\n",
      "11    16869\n",
      "12    16870\n",
      "13    16871\n",
      "14    16872\n",
      "15    16873\n",
      "16    16874\n",
      "17    16875\n",
      "18    16876\n",
      "19    16877\n",
      "Name: index, dtype: int64\n",
      "0     6502\n",
      "1     6503\n",
      "2     6504\n",
      "3     6505\n",
      "4     6506\n",
      "5     6507\n",
      "6     6508\n",
      "7     6509\n",
      "8     6510\n",
      "9     6511\n",
      "10    6512\n",
      "11    6513\n",
      "12    6514\n",
      "13    6515\n",
      "14    6516\n",
      "15    6517\n",
      "16    6518\n",
      "17    6519\n",
      "18    6520\n",
      "19    6521\n",
      "Name: index, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show (sample of) indices\n",
    "print(indices[0][0][:20])\n",
    "print(indices[0][1][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f730c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Number of indices in train: 28876\n",
      "Number of indices in test : 7178\n",
      "Number of equal indices: 0\n",
      "Fold: 1\n",
      "Number of indices in train: 28807\n",
      "Number of indices in test : 7247\n",
      "Number of equal indices: 0\n",
      "Fold: 2\n",
      "Number of indices in train: 28910\n",
      "Number of indices in test : 7144\n",
      "Number of equal indices: 0\n",
      "Fold: 3\n",
      "Number of indices in train: 28805\n",
      "Number of indices in test : 7249\n",
      "Number of equal indices: 0\n",
      "Fold: 4\n",
      "Number of indices in train: 28818\n",
      "Number of indices in test : 7236\n",
      "Number of equal indices: 0\n"
     ]
    }
   ],
   "source": [
    "validate_fold_indices(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf280f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
