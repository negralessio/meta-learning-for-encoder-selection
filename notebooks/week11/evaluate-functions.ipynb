{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2a2dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ef08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.load_datasets import load_dataset, load_rankings, load_train_data\n",
    "import src.evaluate_regression\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e6364",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5ac93",
   "metadata": {},
   "source": [
    "## Given functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40aa2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_validated_indices(df: pd.DataFrame, factors: Iterable[str], target: str,\n",
    "                                   **kfoldargs) -> List[List[Iterable[int]]]:\n",
    "    df_factors = df.groupby(factors)[target].mean().reset_index()\n",
    "    X_factors, y_factors = df_factors.drop(target, axis=1), df_factors[target]\n",
    "\n",
    "    indices = []\n",
    "    for itr, ite in KFold(**kfoldargs).split(X_factors, y_factors):\n",
    "        tr = pd.merge(X_factors.iloc[itr], df.reset_index(), on=factors).index  # \"index\" is the index of df\n",
    "        te = pd.merge(X_factors.iloc[ite], df.reset_index(), on=factors).index  # \"index\" is the index of df\n",
    "        indices.append([tr, te])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64e19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rankings(df: pd.DataFrame, factors, new_index, target) -> pd.DataFrame:\n",
    "\n",
    "    assert set(factors).issubset(df.columns)\n",
    "\n",
    "    rankings = {}\n",
    "    for group, indices in df.groupby(factors).groups.items():\n",
    "        score = df.iloc[indices].set_index(new_index)[target]\n",
    "        rankings[group] = score2ranking(score, ascending=False)\n",
    "\n",
    "    return pd.DataFrame(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ada21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2ranking(score: pd.Series, ascending=True):\n",
    "    \"\"\"\n",
    "    Ascending =\n",
    "        True: lower score = better rank (for instance, if score is the result of a loss function or a ranking itself)\n",
    "        False: greater score = better rank (for instance, if score is the result of a score such as roc_auc_score)\n",
    "    \"\"\"\n",
    "    c = 1 if ascending else -1\n",
    "    order_map = {\n",
    "        s: sorted(score.unique(), key=lambda x: c * x).index(s) for s in score.unique()\n",
    "    }\n",
    "    return score.map(order_map)\n",
    "\n",
    "\n",
    "def spearman_rho(x: Iterable, y: Iterable, nan_policy=\"omit\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)\n",
    "        return spearmanr(x, y, nan_policy=nan_policy)[0]\n",
    "\n",
    "\n",
    "def list_spearman(rf1: pd.DataFrame, rf2: pd.DataFrame) -> np.array:\n",
    "    if not rf1.columns.equals(rf2.columns) or not rf1.index.equals(rf2.index):\n",
    "        raise ValueError(\"The two input dataframes should have the same index and columns.\")\n",
    "\n",
    "    return np.array([\n",
    "        spearman_rho(r1, r2, nan_policy=\"omit\") for (_, r1), (_, r2) in zip(rf1.items(), rf2.items())\n",
    "    ])\n",
    "\n",
    "\n",
    "def average_spearman(rf1: pd.DataFrame, rf2: pd.DataFrame) -> np.array:\n",
    "    #with warnings.catch_warnings():\n",
    "    #    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    return np.nanmean(list_spearman(rf1, rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125e092",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d798f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_cross_validated_indices(df: pd.DataFrame, factors, target: str,\n",
    "                                   **kfoldargs):\n",
    "    df_factors = df.groupby(factors)[target].mean().reset_index()\n",
    "    X_factors, y_factors = df_factors.drop(target, axis=1), df_factors[target]\n",
    "\n",
    "    indices = []\n",
    "    for itr, ite in KFold(**kfoldargs).split(X_factors, y_factors):\n",
    "        tr = pd.merge(X_factors.iloc[itr], df.reset_index(), on=factors)['index']  # \"index\" is the index of df\n",
    "        te = pd.merge(X_factors.iloc[ite], df.reset_index(), on=factors)['index']  # \"index\" is the index of df\n",
    "        indices.append([tr, te])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4709375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_rankings(df: pd.DataFrame, factors, new_index, target) -> pd.DataFrame:\n",
    "\n",
    "    assert set(factors).issubset(df.columns)\n",
    "\n",
    "    rankings = {}\n",
    "    #print(df.shape)\n",
    "    for group, indices in df.groupby(factors).groups.items():\n",
    "        #print(indices)\n",
    "        #print(df.shape)\n",
    "        score = df.loc[indices].set_index(new_index)[target]\n",
    "        rankings[group] = src.evaluate_regression.score2ranking(score, ascending=False)\n",
    "\n",
    "    return pd.DataFrame(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bc4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fold_indices(indices):\n",
    "    for i in range(len(indices)):\n",
    "        # Train set\n",
    "        print(f\"Fold: {i}\")\n",
    "        print(f\"Number of indices in train: {len(indices[i][0])}\")\n",
    "        print(f\"Number of indices in test : {len(indices[i][1])}\")\n",
    "\n",
    "\n",
    "        intersect = np.intersect1d(indices[i][0], indices[i][1])\n",
    "        print(f\"Number of equal indices: {len(intersect)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6f4b3",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda975e",
   "metadata": {},
   "source": [
    "## Given functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad926af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from '../../data/raw/dataset_rank_train.csv'...\n",
      "0.9878347093031644\n",
      "0.9885479266293551\n",
      "0.9886113749126121\n",
      "0.9890837654740263\n",
      "0.9879439989631907\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_train_data('../../data/raw/dataset_rank_train.csv')\n",
    "X_train = X_train.drop(\"rank\", axis=1)\n",
    "\n",
    "# Define variables for ranking\n",
    "factors = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "new_index = \"encoder\"\n",
    "target = \"cv_score\"\n",
    "\n",
    "# Get indices for CV\n",
    "indices = custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "#indices = my_custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "\n",
    "for fold in indices:\n",
    "    # Define data of current fold\n",
    "    X_train_fold = X_train.iloc[fold[0]]\n",
    "    X_test_fold = X_train.iloc[fold[1]]\n",
    "    y_train_fold = y_train.iloc[fold[0]]\n",
    "    y_test_fold = y_train.iloc[fold[1]]\n",
    "    \n",
    "    # Train model and predict\n",
    "    dummy_pipe = Pipeline([(\"encoder\", OneHotEncoder()), (\"model\", DecisionTreeRegressor())])\n",
    "    y_pred = pd.Series(dummy_pipe.fit(X_train_fold, y_train_fold).predict(X_test_fold), index=y_test_fold.index, name=\"cv_score_pred\")\n",
    "    df_pred = pd.concat([X_test_fold, y_test_fold, y_pred], axis=1)\n",
    "    \n",
    "    # Rankings and avg_spearman\n",
    "    rankings_test = get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score\")\n",
    "    rankings_pred = get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score_pred\")\n",
    "    print(src.evaluate_regression.average_spearman(rankings_test, rankings_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4308d359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RangeIndex(start=0, stop=28876, step=1),\n",
       " RangeIndex(start=0, stop=7178, step=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show indices\n",
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06de8461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Number of indices in train: 28876\n",
      "Number of indices in test : 7178\n",
      "Number of equal indices: 7178\n",
      "Fold: 1\n",
      "Number of indices in train: 28807\n",
      "Number of indices in test : 7247\n",
      "Number of equal indices: 7247\n",
      "Fold: 2\n",
      "Number of indices in train: 28910\n",
      "Number of indices in test : 7144\n",
      "Number of equal indices: 7144\n",
      "Fold: 3\n",
      "Number of indices in train: 28805\n",
      "Number of indices in test : 7249\n",
      "Number of equal indices: 7249\n",
      "Fold: 4\n",
      "Number of indices in train: 28818\n",
      "Number of indices in test : 7236\n",
      "Number of equal indices: 7236\n"
     ]
    }
   ],
   "source": [
    "validate_fold_indices(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa0806",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc49151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from '../../data/raw/dataset_rank_train.csv'...\n",
      "0.4154733195390613\n",
      "0.5134367647902407\n",
      "0.49402199170232297\n",
      "0.501795990639741\n",
      "0.4635566000931501\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_train_data('../../data/raw/dataset_rank_train.csv')\n",
    "X_train = X_train.drop(\"rank\", axis=1)\n",
    "\n",
    "# Define variables for ranking\n",
    "factors = [\"dataset\", \"model\", \"tuning\", \"scoring\"]\n",
    "new_index = \"encoder\"\n",
    "target = \"cv_score\"\n",
    "\n",
    "# Get indices for CV\n",
    "#indices = custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "indices = my_custom_cross_validated_indices(pd.concat([X_train, y_train], axis=1), factors, target, n_splits=5, shuffle=True, random_state=1444)\n",
    "\n",
    "for fold in indices:\n",
    "    # Define data of current fold\n",
    "    X_train_fold = X_train.iloc[fold[0]]\n",
    "    X_test_fold = X_train.iloc[fold[1]]\n",
    "    y_train_fold = y_train.iloc[fold[0]]\n",
    "    y_test_fold = y_train.iloc[fold[1]]\n",
    "    \n",
    "    # Train model and predict\n",
    "    dummy_pipe = Pipeline([(\"encoder\", OneHotEncoder()), (\"model\", DecisionTreeRegressor())])\n",
    "    y_pred = pd.Series(dummy_pipe.fit(X_train_fold, y_train_fold).predict(X_test_fold), index=y_test_fold.index, name=\"cv_score_pred\")\n",
    "    df_pred = pd.concat([X_test_fold, y_test_fold, y_pred], axis=1)\n",
    "    \n",
    "    # Rankings and avg_spearman\n",
    "    rankings_test = my_get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score\")\n",
    "    rankings_pred = my_get_rankings(df_pred, factors=factors, new_index=new_index, target=\"cv_score_pred\")\n",
    "    print(src.evaluate_regression.average_spearman(rankings_test, rankings_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645909fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
